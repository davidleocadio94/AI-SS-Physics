# an example config for the tutorial
# for a config with good default choices, please see configs/example.yaml in the nequip repo
# Two folders will be used during the training: 'root'/process and 'root'/'run_name'
# run_name contains logfiles and saved models
# process contains processed data sets
# if 'root'/'run_name' exists, 'root'/'run_name'_'year'-'month'-'day'-'hour'-'min'-'s' will be used instead.
root: tutorial-results 
workdir: tutorial-results/requeue
run_name: example-run
seed: 0                                                                           # random number seed for numpy and torch
requeue: true                                                                    # set True for a restarted run
append: true 
default_dtype: float32                                                            # type of float, e.g. float32 and float64
# network
compile_model: False                                                              # whether to compile the constructed model to TorchScript
num_basis: 8                                                                      # number of basis functions
r_max: 6                                                                          # cutoff radius
irreps_edge_sh: 1x0e + 1x1o + 1x2e  #0e + 1o                                                           # irreps of the spherical harmonics used for edges. If a single integer, indicates the full SH up to L_max=that_integer
conv_to_output_hidden_irreps_out: 16x0e                                            # irreps used in hidden layer of output block
chemical_embedding_irreps_out: 16x0e                                               # irreps of the chemical feature embedding
feature_irreps_hidden: 16x0o + 16x0e + 16x1o + 16x1e + 16x2o + 16x2e #+ 32x3o + 32x3e #8x0o + 8x0e + 8x1o + 8x1e                                  # irreps used for hidden features, here we go up to lmax=2, with even and odd parities
BesselBasis_trainable: true                                                       # set true to train the bessel weights
nonlinearity_type: gate                                                           # may be 'gate' or 'norm', 'gate' is recommended
num_layers: 5                                                                     # number of interaction blocks, we found 5-6 to work best
resnet: true                                                                     # set True to make interaction block a resnet-style update
PolynomialCutoff_p: 6                                                             # p-value used in polynomial cutoff function
invariant_layers: 2                                                               # number of radial layers, we found it important to keep this small, 1 or 2
invariant_neurons: 64                                                              # number of hidden neurons in radial function, again keep this small for MD applications, 8 - 32, smaller is faster
avg_num_neighbors: null                                                           # number of neighbors to divide by, None => no normalization.
use_sc: true                                                                      # use self-connection or not, usually gives big improvement
# data set
# the keys used need to be stated at least once in key_mapping, npz_fixed_field_keys or npz_keys
# key_mapping is used to map the key in the npz file to the NequIP default values (see data/_key.py)
# all arrays are expected to have the shape of (nframe, natom, ?) except the fixed fields
dataset: npz                                                                       # type of data set, can be npz or ase
dataset_url: #http://quantum-machine.org/gdml/data/npz/benzene_ccsdt_t-train.zip             # url to download the npz. optional
dataset_file_name: ./tutorial_data/trajectory.npz #./tutorial_data/benzene_ccsd_t-train.npz                          # path to data set file
key_mapping:
  z: atomic_numbers                                                                # atomic species, integers
  E: total_energy                                                                  # total potential eneriges to train to
  F: forces                                                                        # atomic forces to train to
  R: pos                                                                           # raw atomic positions
  CELL: cell
  PBC: pbc
npz_fixed_field_keys:                                                              # fields that are repeated across different examples
  - atomic_numbers
  - cell
  - pbc
chemical_symbol_to_type:
    Sr: 0
    Ti: 1
    O:  2
# logging
wandb: false                                                                       # we recommend using wandb for logging, we'll turn it off here as it's optional
                                                                                   # if False, a new wandb run will be generated
verbose: info                                                                      # the same as python logging, e.g. warning, info, debug, error. case insensitive
log_batch_freq: 5                                                                  # batch frequency, how often to print training errors withinin the same epoch
log_epoch_freq: 1                                                                  # epoch frequency, how often to print and save the model
# scalar nonlinearities to use â€” available options are silu, ssp (shifted softplus), tanh, and abs.
# Different nonlinearities are specified for e (even) and o (odd) parity;
# note that only tanh and abs are correct for o (odd parity).
nonlinearity_scalars:
  e: silu
  o: tanh
nonlinearity_gates:
  e: silu
  o: tanh
# training
n_train: 100                                                                        # number of training data
n_val: 3000                                                                          # number of validation data
learning_rate: 0.01                                                                # learning rate, we found 0.01 to work best - this is often one of the most important hyperparameters to tune
batch_size: 2                                                                      # batch size, we found it important to keep this small for most applications 
max_epochs: 60                                                                     # stop training after _ number of epochs
train_val_split: random                                                            # can be random or sequential. if sequential, first n_train elements are training, next n_val are val, else random 
shuffle: true                                                                      # If true, the data loader will shuffle the data
metrics_key: validation_loss                                                                  # metrics used for scheduling and saving best model. Options: loss, or anything that appears in the validation batch step header, such as f_mae, f_rmse, e_mae, e_rmse
use_ema: True                                                                     # if true, use exponential moving average on weights for val/test
ema_decay: 0.999                                                                  # ema weight, commonly set to 0.999
ema_use_num_updates: True                                                          # whether to use number of updates when computing averages
# loss function
loss_coeffs:                                                                       # different weights to use in a weighted loss functions
 forces: 100                                                                        # for MD applications, we recommed a force weight of 100 and an energy weight of 1
 total_energy:                                                                    # alternatively, if energies are not of importance, a force weight 1 and an energy weight of 0 also works.
    - 1
    - PerAtomMSELoss
# output metrics
# early stopping based on metrics values.
early_stopping_patiences:                                                          # stop early if a metric value stopped decreasing for n epochs
  validation_loss: 50
metrics_components:
  - - forces                               # key
    - rmse                                 # "rmse" or "mse"
      #- PerSpecies: False                     # if true, per species contribution is counted separately
    - report_per_component: True          # if true, statistics on each component (i.e. fx, fy, fz) will be counted separately
  - - forces
    - mae
      #- PerSpecies: False
    - report_per_component: True
  - - total_energy
    - mae
    - PerAtom: True                        # if true, energy is normalized by the number of atoms
  - - total_energy
    - rmse
    - PerAtom: True
      
# the name `optimizer_name`is case sensitive
optimizer_name: Adam                                                               # default optimizer is Adam in the amsgrad mode
optimizer_amsgrad: true
optimizer_betas: !!python/tuple
  - 0.9
  - 0.999
optimizer_eps: 1.0e-08
optimizer_weight_decay: 0
# lr scheduler
lr_scheduler_name: ReduceLROnPlateau
lr_scheduler_patience: 50
lr_scheduler_factor: 0.5
# we provide a series of options to shift and scale the data
# these are for advanced use and usually the defaults work very well
# the default is to scale the atomic energy and forces by scaling them by the force standard deviation and to shift the energy by the mean atomic energy
# in certain cases, it can be useful to have a trainable shift/scale and to also have species-dependent shifts/scales for each atom
#per_species_rescale_shifts_trainable: true
#per_species_rescale_scales_trainable: true
# whether the shifts and scales are trainable. Defaults to False. Optional
#per_species_rescale_shifts: dataset_per_atom_total_energy_mean
# initial atomic energy shift for each species. default to the mean of per atom energy. Optional
# the value can be a constant float value, an array for each species, or a string that defines a statistics over the training dataset
#per_species_rescale_scales: dataset_forces_rms
# initial atomic energy scale for each species. Optional.
# the value can be a constant float value, an array for each species, or a string
# per_species_rescale_arguments_in_dataset_units: True
# if explicit numbers are given for the shifts/scales, this parameter must specify whether the given numbers are unitless shifts/scales or are in the units of the dataset. If ``True``, any global rescalings will correctly be applied to the per-species values.
# # full block needed for per specie rescale
global_rescale_shift: null
global_rescale_shift_trainable: false
global_rescale_scale: dataset_forces_rms
global_rescale_scale_trainable: false
per_species_rescale_trainable: true
per_species_rescale_shifts: dataset_per_atom_total_energy_mean
per_species_rescale_scales: dataset_per_atom_total_energy_std
