#!/bin/bash -l
# Standard output and error:
#SBATCH -o ./job.out.%j
#SBATCH -e ./job.err.%j
# Initial working directory:
#SBATCH -D ./
# Job name
#SBATCH -J +15
#
#SBATCH --ntasks=1
#SBATCH --constraint="gpu"
#
# --- default case: use a single GPU on a shared node ---
#SBATCH --gres=gpu:a100:1
#SBATCH --cpus-per-task=18
#SBATCH --mem=125000
#
# --- uncomment to use 2 GPUs on a shared node ---
# #SBATCH --gres=gpu:a100:2
# #SBATCH --cpus-per-task=36
# #SBATCH --mem=250000
#
# --- uncomment to use 4 GPUs on a full node ---
# #SBATCH --gres=gpu:a100:4
# #SBATCH --cpus-per-task=72
# #SBATCH --mem=500000
#
#SBATCH --mail-type=none
#SBATCH --mail-user=userid@example.mpg.de
#SBATCH --time=22:30:00


module purge
module load intel/21.3.0
module load impi/2021.3
module load mkl/2021.3
module load cuda/11.4
module load cudnn/8.2.4
module load git/2.31
module load anaconda/3/2021.05
module load gcc/11
module load pytorch/gpu-cuda-11.2/1.10.0
module load cmake
module list

export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}

srun ~/lammps/build/lmp -in in.sto
